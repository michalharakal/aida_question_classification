# -*- coding: utf-8 -*-
"""Group_A_AN_EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OrZzW2bYvG7JQF71w2q3qZ-zlqNQqlUf

EDA

https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label

https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label
"""

# %%
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd


def process_question(row):
    return " ".join(row.split(" ")[1:])


train_df = pd.read_table("https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label", encoding="ISO-8859-1",
                         header=None)
train_df.columns = ["raw"]
train_df['category'] = train_df.apply(lambda row: row["raw"].split(":")[0], axis=1)
train_df['subcategory'] = train_df.apply(lambda row: row["raw"].split(" ")[0].split(":")[1], axis=1)
train_df['question'] = train_df.apply(lambda row: process_question(row["raw"]), axis=1)

train_df

train_df.nunique()

train_df.shape

train_df.head(5)

train_df.columns

import pandas as pd


def process_question(row):
    return " ".join(row.split(" ")[1:])


test_df = pd.read_table("https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label", encoding="ISO-8859-1", header=None)
test_df.columns = ["raw"]
test_df['category'] = train_df.apply(lambda row: row["raw"].split(":")[0], axis=1)
test_df['subcategory'] = train_df.apply(lambda row: row["raw"].split(" ")[0].split(":")[1], axis=1)
test_df['question'] = train_df.apply(lambda row: process_question(row["raw"]), axis=1)

test_df

train_df.category.value_counts()

train_df.category.value_counts().plot.pie()

test_df.category.value_counts()

test_df.category.value_counts().plot.pie()

test_df.nunique()

train_df.subcategory.groupby(train_df['category']).value_counts()

test_df.subcategory.groupby(test_df['category']).value_counts()

# test_df with only 38 subcategories compared to 47 in train_df

n = 14  # input for nlargest
train_df.subcategory.value_counts().nlargest(n).sum()

p = (train_df.subcategory.value_counts().nlargest(n).sum()) / len(train_df)
print(f'the {n} largest subcategories account for a proportion of {p}')

train_df.subcategory.value_counts().nlargest(n)

top_subcategories = list((train_df.subcategory.value_counts().nlargest(n)).index)
top_subcategories

train_df_top = train_df.loc[train_df['subcategory'].isin(top_subcategories)]
len(train_df_top)

train_df_top.sample(10)

train_df_top.subcategory.groupby(train_df['category']).value_counts()

# More than 80% of the data can be assigned to only 14 out of 47 subcategories in train_df


test_df.subcategory.value_counts().nlargest(39)


if __name__ == '__main__' :
    main()
