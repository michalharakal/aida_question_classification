{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCyCaq9xV5Tp"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOywAUdvWAC2"
   },
   "source": [
    "https://cogcomp.seas.upenn.edu/Data/QA/QC/definition.html                                        \n",
    "https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label                                               \n",
    "https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#petras_path for plots \n",
    "path_plot = '/home/petra42/AIDA_Abschluss/aida_question_classification/plots/'\n",
    "#path_plot = '/home/an/aida_question_classification/plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pOQMVj-BHy9"
   },
   "source": [
    "## Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-LddWxFSBG5f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-c701f1df517d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#visualisation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_class_def  = 'https://cogcomp.seas.upenn.edu/Data/QA/QC/definition.html'\n",
    "path_train_data = 'https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label'\n",
    "path_test_data  = 'https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtANgZNdV37o",
    "outputId": "b2960677-d8e0-4c5d-a546-fd20af8e6e0e"
   },
   "outputs": [],
   "source": [
    "def process_question(row):\n",
    "   return \" \".join(row.split(\" \")[1:])\n",
    "\n",
    "train_df = pd.read_table(path_train_data, encoding = \"ISO-8859-1\", header=None)\n",
    "train_df.columns = [\"raw\"]\n",
    "train_df['category'] = train_df.apply (lambda row: row[\"raw\"].split(\":\")[0], axis=1)\n",
    "train_df['subcategory'] = train_df.apply (lambda row: row[\"raw\"].split(\" \")[0].split(\":\")[1], axis=1)\n",
    "train_df['question'] = train_df.apply (lambda row: process_question(row[\"raw\"]), axis=1)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "fwT9pkAZZ5-7",
    "outputId": "6311d7f0-2574-4fc6-8c89-5b6d930c13a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_table(path_test_data, encoding = \"ISO-8859-1\", header=None)\n",
    "test_df.columns = [\"raw\"]\n",
    "test_df['category'] = train_df.apply (lambda row: row[\"raw\"].split(\":\")[0], axis=1)\n",
    "test_df['subcategory'] = train_df.apply (lambda row: row[\"raw\"].split(\" \")[0].split(\":\")[1], axis=1)\n",
    "test_df['question'] = train_df.apply (lambda row: process_question(row[\"raw\"]), axis=1)\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)\n",
    "\n",
    "#describe\n",
    "train_df.describe()\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E83DrFnjw9Y3"
   },
   "source": [
    "\n",
    "## question:\n",
    "    - shape \n",
    "    - size\n",
    "    - info: \n",
    "        count: row, unique categories, subcategories\n",
    "    - witch catagories, subcategories, same in both dataframes\n",
    "    - distribution\n",
    "    - len questions (count token)\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print test, train shape\n",
    "print(f'---shapes---\\ntrain:\\t{train_df.shape}\\ntest:\\t{test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of row of columns\n",
    "print(f'train_size:\\t{train_df.size}\\ntest_size:\\t{test_df.size}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line occupancy    \n",
    "print(f'---train---:\\n {train_df.nunique()}\\n')\n",
    "print(f'---test---:\\n {test_df.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which categories?\n",
    "print(f\"unique categories: {train_df['category'].unique()}\")\n",
    "#same categories in train and test?\n",
    "train_df['category'].unique() == test_df['category'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary as dict\n",
    "#which categories are not used?.\n",
    "#which categories are used in train and test?\n",
    "\n",
    "print(f\"unique subcategories: {train_df['subcategory'].unique()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution categories (train)\n",
    "dist_train_cat = train_df.groupby('category')['subcategory'].count()\n",
    "print(dist_train_cat)\n",
    "dist_train_cat.sort_values(ascending=False)\n",
    "\n",
    "#visualisation \n",
    "dist_train_cat.plot.pie()\n",
    "plt.title('Distribution categories (train)')\n",
    "plt.savefig(path_plot + 'Distribution_categories_train.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist_train_cat)\n",
    "dist_train_cat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(dist_train_cat,labels=dist_train_cat.index, autopct='%1.0f%%', pctdistance=1.15, labeldistance=1.3) \n",
    "plt.title('Distribution categories (train)')\n",
    "plt.savefig(path_plot + 'Distribution_categories_train%.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart, where the slices will show the max_category:\n",
    "labels=dist_train_cat.index\n",
    "sizes = dist_train_cat\n",
    "explode = (0, 0, 0.1, 0, 0, 0)  # only \"explode\" the 3nd slice (max_category)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(dist_train_cat, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Distribution categories (train)')\n",
    "plt.savefig(path_plot + 'Distribution_categories_train_slice.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution categories (test)\n",
    "dist_test_cat = test_df.groupby('category')['subcategory'].count()\n",
    "dist_test_cat.sort_values(ascending=False)\n",
    "print(dist_test_cat)\n",
    "\n",
    "plt.pie(dist_test_cat,labels=dist_test_cat.index, autopct='%1.0f%%', pctdistance=1.15, labeldistance=1.3) \n",
    "plt.title('Distribution categories (test)')\n",
    "plt.savefig(path_plot + 'Distribution_categories_test%.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart, where the slices will show the max_categories:\n",
    "labels=dist_test_cat.index\n",
    "explode = (0, 0.1, 0.1, 0, 0, 0)  # only \"explode\" the slice (max_categories)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(dist_test_cat, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Distribution categories (test)')\n",
    "plt.savefig(path_plot + 'Distribution_categories_test_slice.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution subcategories (train)\n",
    "dist_train_sub = train_df.subcategory.groupby(train_df['category']).value_counts()\n",
    "print(dist_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 20 subcategories\n",
    "print(f'first 20: {dist_train_sub.sort_values(ascending=False)[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eGX-t4toQvK",
    "outputId": "5ac99bd6-ffb7-44a8-b3df-cd39c03c10c9"
   },
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(12,6))\n",
    "dist_train_sub.sort_values(ascending=False).plot.bar(label='train')\n",
    "plt.xlabel('subcategories')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Distribution subcategories (train)')\n",
    "plt.savefig(path_plot + 'Distribution_subcategories_train.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peTqIjxGpkIW",
    "outputId": "a3fccc6a-6e4b-4f90-a981-ca55aed6a778"
   },
   "outputs": [],
   "source": [
    "# distribution subcategories (test)\n",
    "dist_test_sub = test_df.subcategory.groupby(test_df['category']).value_counts()\n",
    "print(dist_test_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "AzblExVaohF7",
    "outputId": "c6a661a6-afd4-436a-e935-022f8a803b55"
   },
   "outputs": [],
   "source": [
    "print(f'first 20: {dist_train_sub.sort_values(ascending=False)[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "eKNnqt2LCeRh",
    "outputId": "4eace0a0-ca56-4748-f83b-07c8a2399706"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "dist_test_sub.sort_values(ascending=False).plot.bar(label='test')\n",
    "plt.xlabel('subcategories')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Distribution subcategories (test)')\n",
    "plt.savefig(path_plot + 'Distribution_subcategories_test.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPwtZ9-lszTb",
    "outputId": "044fb10e-e76c-464d-d13d-e87c5404f2ed"
   },
   "outputs": [],
   "source": [
    "#plt.gca().set_xticklabels(xtickvals[::6], rotation=90, fontdict={'horizontalalignment': 'center', 'verticalalignment': 'center_baseline'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "RPsfzSrdtUu3",
    "outputId": "71646412-847c-4ed3-8eb8-0f6504d754ee"
   },
   "outputs": [],
   "source": [
    "#test_df with only 38 subcategories compared to 47 in train_df\n",
    "n = 14   #input for nlargest\n",
    "train_df.subcategory.value_counts().nlargest(n).sum()\n",
    "\n",
    "p = (train_df.subcategory.value_counts().nlargest(n).sum())/len(train_df)\n",
    "print(f'The {n} largest subcategories account for a proportion of {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vts8uXHDDQg7",
    "outputId": "0e7c7326-2e32-45c8-c85f-4c325d96af21"
   },
   "outputs": [],
   "source": [
    "train_df.subcategory.value_counts().nlargest(n)\n",
    "\n",
    "top_subcategories = list((train_df.subcategory.value_counts().nlargest(n)).index)\n",
    "top_subcategories\n",
    "\n",
    "train_df_top = train_df.loc[train_df['subcategory'].isin(top_subcategories)]\n",
    "print(f'The {n} largest subcategories have {len(train_df_top)} questions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "r8CPOMSQuqTT",
    "outputId": "6b620f3a-7dd3-442b-9ad3-35e0dfe95df8"
   },
   "outputs": [],
   "source": [
    "train_df_top.sample(10)\n",
    "train_df_top.subcategory.groupby(train_df['category']).value_counts()\n",
    "\n",
    "#More than 80% of the data can be assigned to only 14 out of 47 subcategories in train_df\n",
    "\n",
    "test_df.subcategory.value_counts().nlargest(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_length_text(df, title, diagram):\n",
    "    '''\n",
    "    creates a histogram \n",
    "    that shows the distribution of the number of tokens \n",
    "    per line for the data set to be examined\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        series; to examined text\n",
    "    title: \n",
    "        string; title for Plot\n",
    "    diagram: \n",
    "        string: name of the diagram\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Vizualisationplot Distribution\n",
    "    '''\n",
    "    plt.hist(df.apply(lambda text: len(text.split())))\n",
    "    plt.xlabel('number of token')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title(title)\n",
    "    plt.savefig(path_plot + diagram + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    avg = round(df.apply(lambda text: len(text.split())).mean())\n",
    "    maxi = max(list(df.apply(lambda text: len(text.split()))))\n",
    "    mini = min(list(df.apply(lambda text: len(text.split()))))\n",
    "    \n",
    "    print(f'The texts have a mean length of {avg} tokens.')\n",
    "    print(f'The longest text has {maxi} tokens, the shortest has {mini} tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of length of questions, train data\n",
    "df = train_df['question']\n",
    "title = 'Distribution of length of questions (train)'\n",
    "diagram ='length_questions_train'\n",
    "\n",
    "get_plot_length_text(df, title, diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of length of questions, test data\n",
    "df = test_df['question']\n",
    "title = 'Distribution of length of questions (test)'\n",
    "diagram ='length_questions_test'\n",
    "\n",
    "get_plot_length_text(df, title, diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of length of questions, train and test data\n",
    "ax1 = plt.hist(train_df['question'].apply(lambda text: len(text.split())),label='train')\n",
    "ax2 = plt.hist(test_df['question'].apply(lambda text: len(text.split())), label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('number of token')\n",
    "plt.ylabel('frequencies per data set')\n",
    "plt.title('Distributions of length of questions')\n",
    "plt.savefig(path_plot + 'length_questions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average length of question\n",
    "al_train = round(train_df['question'].apply(lambda text: len(text.split())).mean())\n",
    "al_test = round(test_df['question'].apply(lambda text: len(text.split())).mean())\n",
    "print(f'The questions in the train data set have an average of {al_train} tokens,\\\n",
    " and {al_test} tokens in the test data set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one token per cell --- from question\n",
    "train_question_df = train_df['question'].str.split(\" \", expand=True)\n",
    "test_question_df = test_df['question'].str.split(\" \", expand=True)\n",
    "\n",
    "# print test, train shape\n",
    "print(f'---shapes---\\ntrain_df:\\t{train_df.shape}\\\n",
    "\\ntrain_question:\\t{train_question_df.shape}\\n\\\n",
    "test_question:\\t{test_question_df.shape}')\n",
    "\n",
    "train_question_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#percentage distribution of tokens (train)\n",
    "for column in train_question_df.columns:\n",
    "    nan_sum = train_question_df[column].isnull().sum()\n",
    "    print(column,': ',round(100.00-(nan_sum*100/len(train_question_df)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage distribution of tokens (test)\n",
    "for column in test_question_df.columns:\n",
    "    nan_sum = test_question_df[column].isnull().sum()\n",
    "    print(column,': ',round(100.00-(nan_sum*100/len(test_question_df)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of most frequent first words in questions - top 15\n",
    "question_top_words = train_question_df[0].value_counts().nlargest(15)\n",
    "question_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_words = list(train_question_df[0].value_counts().nlargest(15).index[0:6])\n",
    "question_words  #top 7 from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination of words table with (sub)categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_words = train_df.join(train_question_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "cfZ4fcAtHEro",
    "outputId": "ac6f25e3-b3a8-488e-eb10-71e8841c1d21"
   },
   "outputs": [],
   "source": [
    "train_df_words_top = train_df_words.loc[train_df_words[0].isin(question_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More than 90% of the question texts start with one of 7 (top) question words\n",
    "train_df_words_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question words give indications for category\n",
    "train_df_words_top[0].groupby(train_df_words_top['category']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_words_top   only used for eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv2Chs50IV7x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find duplicates in the raw text (category, subcategory and question)  ==> 100%match\n",
    "\n",
    "def clean_duplicates(df, column):\n",
    "    '''\n",
    "    Find duplicates and drop by 100% match\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df; name of the DataFrame\n",
    "    column; to column to be examined \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    series; to check whether duplicates present\n",
    "        df; clean df only duplicates\n",
    "    '''\n",
    "    duplicat = list(df[column].duplicated())\n",
    "    print (set(duplicat))\n",
    "    \n",
    "    count_dups = df.pivot_table(index=['raw'], aggfunc='size')\n",
    "    nb = len(df[column])-len(count_dups)\n",
    "    print(f'The column has {nb} duplicates.')\n",
    "    \n",
    "clean_duplicates(train_df, 'raw')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tzNmE69ItJ7",
    "outputId": "1b5d8cd3-9d43-43da-8d1b-b7b3886986b3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first value=> false, secound values =>true\n",
    "dupl_first = list(train_df['raw'].duplicated())\n",
    "print(list([index for index, value in enumerate(list(dupl_first)) if value == True]),'\\n\\n')\n",
    "\n",
    "#first value=> true, secound values =>false\n",
    "dupl_last = list(train_df['raw'].duplicated(keep='last'))\n",
    "print(list([index for index, value in enumerate(list(dupl_last)) if value == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wIP4ttMIvF0",
    "outputId": "8e349553-da91-4d87-8779-b174b441fd3a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sample (manueal) duplicates\n",
    "train_df['raw'].loc[369]\n",
    "train_df.loc[train_df['raw']=='NUM:speed What is the speed of the Mississippi River ?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['raw'].loc[5219]\n",
    "train_df.loc[train_df['raw']=='HUM:ind What Pope inaugurated Vatican International Radio ?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "train_df_dropdup = train_df.drop_duplicates()\n",
    "print(f'shape:\\ntrain_df:\\t\\t{train_df.shape}\\ntrain_df_droped:\\t{train_df_dropdup.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicates in test_df\n",
    "clean_duplicates(test_df,'raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage of questions to shorter text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-a345a0cc79a7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcorpus\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstem\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mWordNetLemmatizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# default setting for lemmatizer and stopwords\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# remove stop words\n",
    "nltk.download('stopwords')\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Regex cleaning of the text. Filters everything except alphanumerical and '.\n",
    "    Return is turned into lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        text to be cleaned\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        lower case regex cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    text = text.replace(\"Â´\", \"'\")\n",
    "    \n",
    "    text = text.replace(\"'s\", \" \")\n",
    "\n",
    "    digi_punct = \"[^a-zA-Z.1234567890#' ]\"\n",
    "    text = re.sub(digi_punct, \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def stopword_text(text):\n",
    "    \"\"\"\n",
    "    Remove all words in the text that are in the stopword list\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        text only stopwords\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([word for word in text.split() if word not in my_stopwords])\n",
    "\n",
    "\n",
    "\n",
    "def lem_text(text):\n",
    "    \"\"\"\n",
    "    Group the different inflected forms of a word so they can be analysed as \n",
    "    a single item\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        text with lemmas\n",
    "    \"\"\"\n",
    "\n",
    "    lem_sentence = text.split()\n",
    "    for i, word in enumerate(text.split()):\n",
    "        for pos in \"n\", \"v\", \"a\", \"r\":\n",
    "            lem = lemmatizer.lemmatize(word, pos=pos)\n",
    "            if lem != word:\n",
    "                lem_sentence[i] = lem\n",
    "                break\n",
    "            else:\n",
    "                lem_sentence[i] = word\n",
    "    return \" \".join(lem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corpus\n",
    "def corpus_func(df):\n",
    "    '''\n",
    "    create a textcorpus from pd.series \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text, string\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    concatinated string with marker ##### as selector\n",
    "    '''\n",
    "    return \"######\".join(text for text in df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus from df_train['question'] \n",
    "corpus = corpus_func(train_df['question'])\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus from df_test['question']\n",
    "corpus_test = corpus_func(test_df['question'])\n",
    "print(corpus_test[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create split-list corpus --> tranfer into list --> dataFrame\n",
    "\n",
    "def corpus_list(corpus):\n",
    "    '''split testcorpus in lines'''\n",
    "    corpus_list = lambda x: x.split('######')\n",
    "\n",
    "#try\n",
    "#test = corpus_list(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords, clean_text (regex, lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check stopwords\n",
    "stopword_text(\"why 's Peter's i I you what How Why go 's me how to school\")  \n",
    "#Attention: question words in LOWER case are in stopwords, in UPPER case not !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus df_train['text'] --> function stopwords\n",
    "text_corpus = stopword_text(corpus) \n",
    "print(text_corpus[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test['text']\n",
    "text_corpus_test = stopword_text(corpus_test)\n",
    "text_corpus_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text column generated from question deleting stopwords\n",
    "train_df['text'] = text_corpus.split('######')      \n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.shape)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = text_corpus_test.split('######') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of length of questions, after stopword-function\n",
    "df = train_df['text']\n",
    "title = 'Distribution of shortened text (train)'\n",
    "diagram ='length_stop_train'\n",
    "\n",
    "get_plot_length_text(df, title, diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text_df = train_df['text'].str.split(\" \", expand=True)\n",
    "train_text_df\n",
    "\n",
    "test_text_df = test_df['text'].str.split(\" \", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage distribution of tokens (train)\n",
    "for column in train_text_df.columns:\n",
    "    nan_sum = train_text_df[column].isnull().sum()\n",
    "    print(column,': ',round(100.00-(nan_sum*100/len(train_text_df)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(train_text_df[16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus df_train['text'] --> function clean_text\n",
    "clean_corpus = clean_text(text_corpus)\n",
    "print(clean_corpus[:500])\n",
    "\n",
    "clean_corpus_test = clean_text(text_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text column generated from question deleting stopwords\n",
    "train_df['text_clean'] = clean_corpus.split('######')      \n",
    "train_df.head(2)\n",
    "\n",
    "test_df['text_clean'] = clean_corpus_test.split('######') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of length of questions, after clean_function\n",
    "df = train_df['text_clean']\n",
    "title = 'Distribution of cleaned,shortened text (train)'\n",
    "diagram ='length_clean_train'\n",
    "\n",
    "get_plot_length_text(df, title, diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "#one token per cell --- from text_clean\n",
    "train_text_clean_df = train_df['text_clean'].str.split(\" \", expand=True)\n",
    "#test_text_clean_df = test_df['question'].str.split(\" \", expand=True)\n",
    "\n",
    "# print test, train shape\n",
    "print(f'---shapes---\\ntrain_df:\\t\\t{train_df.shape}\\\n",
    "\\ntrain_text_clean:\\t{train_text_clean_df.shape}')\n",
    "\n",
    "train_text_clean_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df = train_df.join(train_text_clean_df)\n",
    "train_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df.drop_duplicates(inplace=True)\n",
    "train_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "#one token per cell --- from text_clean\n",
    "test_text_clean_df = test_df['text_clean'].str.split(\" \", expand=True)\n",
    "\n",
    "# print test, train shape\n",
    "print(f'---shapes---\\ntest_df:\\t\\t{test_df.shape}\\\n",
    "\\ntest_text_clean:\\t{test_text_clean_df.shape}')\n",
    "\n",
    "test_text_clean_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_df = test_df.join(test_text_clean_df)\n",
    "test_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_df = test_df.join(test_text_clean_df)\n",
    "print(test_final_df.shape)\n",
    "test_final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_question_df = test_df['question'].str.split(\" \", expand=True)   tbd. to be cleaned with stopwords\n",
    "#test_question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus df_test['text'] --> function stopwords\n",
    "text_corpus_test = stopword_text(corpus_test) \n",
    "print(text_corpus_test[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus df_train['text'] --> function clean_text\n",
    "clean_corpus_test = clean_text(text_corpus_test)\n",
    "print(clean_corpus_test[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df['text'] = text_corpus_test.split('######')      #text column generated from question deleting stopwords\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of length of questions, test_df['text']\n",
    "df = test_df['text']     # anzupassen\n",
    "title = 'Distribution of shortened text (test)'\n",
    "diagram ='length_short_test'\n",
    "\n",
    "get_plot_length_text(df, title, diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage distribution of tokens (test)\n",
    "for column in test_question_df.columns:\n",
    "    nan_sum = test_question_df[column].isnull().sum()\n",
    "    print(column,': ',round(100.00-(nan_sum*100/len(test_question_df)),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te= df_test.drop('text', clean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus\n",
    "def corpus_func(df, column):\n",
    "    '''\n",
    "    create a textcorpus from pd.series\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text, string\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    concatinated string with marker ##### as selector\n",
    "    '''\n",
    "    return \"######\".join(text for text in df)\n",
    "\n",
    "def split_corpus_func(corpus):\n",
    "    '''\n",
    "    create a column from text corpus with marker '#####'\n",
    "    as selector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text, string\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    column in dafa frame\n",
    "    '''\n",
    "    return df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    '''\n",
    "    create new columns in the data frame\n",
    "    new_column: 'text' => cleaned stopwords (english)\n",
    "                'text_clean' => regex, lowercase\n",
    "                'text_lemma' => lemmetized\n",
    "    param: df\n",
    "    returns: df with new columns\n",
    "    '''\n",
    "\n",
    "    corpus = corpus_func(df['question'])\n",
    "    text_corpus = stopword_text(corpus)\n",
    "    df['text'] = split_corpus_func(corpus)\n",
    "    clean_corpus = clean_text(text_corpus)\n",
    "    df['text_clean'] = split_corpus_func(clean_corpus)\n",
    "    lemma = lem_text(clean_corpus)\n",
    "    df['text_lemma'] = split_corpus_func(clean_corpus)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-fa631fc08ea9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpreprocess_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_te\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf_te\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'preprocess_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess_dataframe(df_te)\n",
    "df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EDA_petra.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_test38",
   "language": "python",
   "name": "tf_test38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}