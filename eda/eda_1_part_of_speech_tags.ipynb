{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCyCaq9xV5Tp"
   },
   "source": [
    "# EDA\n",
    "\n",
    "path_plot = '/home/petra42/GIT/aida_question_classification/plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pOQMVj-BHy9"
   },
   "source": [
    "## Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LddWxFSBG5f",
    "outputId": "0043ff9b-2949-4d84-c1bf-2777ed755988"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#nltk   import für eda_1 - preprozessing dataframe\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# default setting for lemmatizer and stopwords\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# remove stop words\n",
    "nltk.download('stopwords')\n",
    "my_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILwLvzVlNG9M"
   },
   "source": [
    "##libaries for part od speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_TdmaSgZ0L-"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#plot spacy\n",
    "from spacy import displacy\n",
    "from spacy.attrs import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyK_A_oaCw85",
    "outputId": "ed8dc2ee-f176-4ab2-a235-ff5706d031bc"
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wd7Fd5RBeYo"
   },
   "source": [
    "##Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4shqn-BxBkPD"
   },
   "outputs": [],
   "source": [
    "# from folder data\n",
    "# import data.get_data.process_question(row)\n",
    "def process_question(row):\n",
    "   '''join row to text''' \n",
    "   text =  \" \".join(row.split(\" \")[1:])\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0Gbeel3Z0L9"
   },
   "outputs": [],
   "source": [
    "def get_plot_length_text(df, title, diagram):\n",
    "    '''\n",
    "    creates a histogram \n",
    "    that shows the distribution of the number of tokens \n",
    "    per line for the data set to be examined\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        series; to examined text\n",
    "    title: \n",
    "        string; title for Plot\n",
    "    diagram: \n",
    "        string: name of the diagram\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Vizualisationplot Distribution\n",
    "    '''\n",
    "    plt.hist(df.apply(lambda text: len(text.split())))\n",
    "    plt.xlabel('number of token')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title(title)\n",
    "    plt.savefig(path_plot + diagram + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    avg = round(df.apply(lambda text: len(text.split())).mean())\n",
    "    maxi = max(list(df.apply(lambda text: len(text.split()))))\n",
    "    mini = min(list(df.apply(lambda text: len(text.split()))))\n",
    "    \n",
    "    print(f'The texts have a mean length of {avg} tokens.')\n",
    "    print(f'The longest text has {maxi} tokens, the shortest has {mini} tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKZvaZG3Z0L-"
   },
   "outputs": [],
   "source": [
    "def corpus_func(df):\n",
    "    '''\n",
    "    create a textcorpus from pd.series, df['column']\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text, string\n",
    "\n",
    "    Returns\n",
    "    ------_\n",
    "    concatinated string with marker --XXX-- as selector\n",
    "    '''\n",
    "    return \" XXX \".join(text for text in df)\n",
    "\n",
    "def split_corpus_func(corpus):\n",
    "    '''\n",
    "    create a column from text corpus with marker ' XXX '\n",
    "    as selector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text, string\n",
    "\n",
    "    Returns\n",
    "    ------_\n",
    "    list of text \n",
    "    '''\n",
    "    list = lambda x: x.split(' XXX ')\n",
    "    return list\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    '''\n",
    "    create new columns in the data frame\n",
    "    new_column: 'text' => cleaned stopwords (english)\n",
    "                'text_clean' => regex, lowercase\n",
    "                'text_lemma' => lemmetized\n",
    "    param: df\n",
    "    returns: df with new columns\n",
    "    '''\n",
    "\n",
    "    corpus = corpus_func(df['question'])\n",
    "    text_corpus = stopword_text(corpus)\n",
    "    df['text'] = split_corpus_func(corpus)\n",
    "    clean_corpus = clean_text(text_corpus)\n",
    "    df['text_clean'] = split_corpus_func(clean_corpus)\n",
    "    lemma = lem_text(clean_corpus)\n",
    "    df['text_lemma'] = split_corpus_func(clean_corpus)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVDfZ_9lZ0L-"
   },
   "outputs": [],
   "source": [
    "#import utils.text_manipulation\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Regex cleaning of the text. Filters everything except alphanumerical and '.\n",
    "    Return is turned into lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        text to be cleaned\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        lower case regex cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    text = text.replace(\"´\", \"'\")\n",
    "    \n",
    "    text = text.replace(\"'s\", \" \")\n",
    "\n",
    "    digi_punct = \"[^a-zA-Z.1234567890#' ]\"\n",
    "    text = re.sub(digi_punct, \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def stopword_text(text):\n",
    "    \"\"\"\n",
    "    Remove all words in the text that are in the stopword list\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        text only stopwords\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([word for word in text.split() if word not in my_stopwords])\n",
    "\n",
    "def lem_text(text):\n",
    "    \"\"\"\n",
    "    Group the different inflected forms of a word so they can be analysed as \n",
    "    a single item\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        text with lemmas\n",
    "    \"\"\"\n",
    "\n",
    "    lem_sentence = text.split()\n",
    "    for i, word in enumerate(text.split()):\n",
    "        for pos in \"n\", \"v\", \"a\", \"r\":\n",
    "            lem = lemmatizer.lemmatize(word, pos=pos)\n",
    "            if lem != word:\n",
    "                lem_sentence[i] = lem\n",
    "                break\n",
    "            else:\n",
    "                lem_sentence[i] = word\n",
    "    return \" \".join(lem_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVvBvJQJBnVS"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiNQ5y7YZ0L9"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "path_class_def  = 'https://cogcomp.seas.upenn.edu/Data/QA/QC/definition.html'\n",
    "path_train_data = 'https://cogcomp.seas.upenn.edu/Data/QA/QC/train_5500.label'\n",
    "path_test_data  = 'https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "vtANgZNdV37o",
    "outputId": "2d507a61-d171-45bb-b576-1a8a11c336bd"
   },
   "outputs": [],
   "source": [
    "#load\n",
    "train_df = pd.read_table(path_train_data, encoding = \"ISO-8859-1\", header=None)\n",
    "train_df.columns = [\"raw\"]\n",
    "train_df['category'] = train_df.apply (lambda row: row[\"raw\"].split(\":\")[0], axis=1)\n",
    "train_df['subcategory'] = train_df.apply (lambda row: row[\"raw\"].split(\" \")[0].split(\":\")[1], axis=1)\n",
    "train_df['question'] = train_df.apply (lambda row: process_question(row[\"raw\"]), axis=1)\n",
    "\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "fwT9pkAZZ5-7",
    "outputId": "bed17648-fa9f-40fb-b43b-c3fcb1fa1009",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_table(path_test_data, encoding = \"ISO-8859-1\", header=None)\n",
    "test_df.columns = [\"raw\"]\n",
    "test_df['category'] = train_df.apply (lambda row: row[\"raw\"].split(\":\")[0], axis=1)\n",
    "test_df['subcategory'] = train_df.apply (lambda row: row[\"raw\"].split(\" \")[0].split(\":\")[1], axis=1)\n",
    "test_df['question'] = train_df.apply (lambda row: process_question(row[\"raw\"]), axis=1)\n",
    "\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZdJgd9aALb-"
   },
   "source": [
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skySeANkZ0L-",
    "outputId": "3f15b66e-958f-4f67-c868-14eee6332304"
   },
   "outputs": [],
   "source": [
    "# corpus from df_train['question'] \n",
    "corpus = corpus_func(train_df['question'])\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qqay-x5MZ0L-",
    "outputId": "de29b3ba-5471-4fa9-ae39-9396f25c6c3e"
   },
   "outputs": [],
   "source": [
    "#corpus from df_test['question']\n",
    "corpus_test = corpus_func(test_df['question'])\n",
    "print(corpus_test[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uitfp2j-Ed2N"
   },
   "outputs": [],
   "source": [
    "text = corpus\n",
    "\n",
    "#sample\n",
    "text_sample = process_question(train_df['raw'][10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E83DrFnjw9Y3"
   },
   "source": [
    "#Part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if_VHlvTCGRv",
    "outputId": "e9a1c69d-5ade-458f-c0ec-e4b33389e422"
   },
   "outputs": [],
   "source": [
    "# Split the data into individual tokens.\n",
    "text_splitted = text.split()\n",
    "\n",
    "\n",
    "# or:\n",
    "word_tokens = word_tokenize(text) \n",
    "\n",
    "print('split:\\t',text_splitted[:100],'\\ntokenize:',word_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyVDXw9wIbpK",
    "outputId": "5060fdf0-f28e-49fe-a3db-3b6c48c25bad"
   },
   "outputs": [],
   "source": [
    "# Tag each token into a specific part of speech \n",
    "word_tagged = nltk.pos_tag(word_tokens)\n",
    "\n",
    "print('tagged:\\t',word_tagged[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZPkvO4TRJYS",
    "outputId": "7578f120-ab43-4376-fc63-bbf47ceea0a9"
   },
   "outputs": [],
   "source": [
    "# Print out the frequency for each part of speech. (i.e. 2 NN's, 100 VBG's, etc.)\n",
    "# To see the acronym definitions for each part of speech,\n",
    "# look at the following link: https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/\n",
    "\n",
    "# with counter from collections\n",
    "word_count = sorted(Counter(word_tagged).items(), key = lambda kv:( kv[1], kv[0]), reverse=True)\n",
    "\n",
    "#or with nltk FreqDist\n",
    "fdist = FreqDist(word_tagged)\n",
    "first20_count = fdist.most_common(10)\n",
    "\n",
    "\n",
    "print(f'word_count:{word_count}\\n len word_count: {len(word_count)}')\n",
    "print(f'first20_count :{first20_count }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "BNaskcG9K6SC",
    "outputId": "6a2ddac0-324b-4477-b684-17dbe210e75f"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text_sample)\n",
    "\n",
    "print('token','-', 'lemma_', '-', 'lower_''-', 'pos_','-', 'tag_-', 'dep_','-', 'sentiment','-','is_alpha','-', 'is_stop')\n",
    "\n",
    "for token in doc:\n",
    "    #any token_attributs; more: https://spacy.io/api/token#attributes\n",
    "    \n",
    "    print('------------------------------------------------------------------------\\n')   \n",
    "    print(token.text,'-', token.lemma_, '-', token.lower_,'-', token.pos_,'-', token.tag_,'-',\n",
    "          token.dep_,'-', token.sentiment,'-', token.is_alpha,'-', token.is_stop)\n",
    "\n",
    "'''\n",
    "Text: The original word text.\n",
    "Lemma: The base form of the word.\n",
    "POS: The simple UPOS part-of-speech tag.\n",
    "Tag: The detailed part-of-speech tag.\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "Shape: The word shape – capitalization, punctuation, digits.\n",
    "is alpha: Is the token an alpha character?\n",
    "is stop: Is the token part of a stop list, i.e. the most common words of the language?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYingoHZVdby",
    "outputId": "ccb66a85-a353-4ac7-f0d9-662303a8f7c9"
   },
   "outputs": [],
   "source": [
    "# Print out the entire spacy text.   --- split by 300 ...to long for display\n",
    "print(doc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvkM10zYVYbi",
    "outputId": "1a13d82d-44c0-4f8c-d8a0-b4f1268f4c90"
   },
   "outputs": [],
   "source": [
    "# check doc is tagged?\n",
    "print(doc.is_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwhWIWmAVTtt",
    "outputId": "29b5c9a0-15fa-40e4-8890-2f86c5349143"
   },
   "outputs": [],
   "source": [
    "#token_tags + explain\n",
    "print([(token.text, token.tag_, spacy.explain(token.tag_)) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTZhtFzoVQJI",
    "outputId": "22377e20-1bb4-4412-e835-3d9dfdb8a5e0"
   },
   "outputs": [],
   "source": [
    "#_entities + label\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1zKrCAGVO86",
    "outputId": "ea0ab216-5e74-4d06-a7bf-2353b4527ba5"
   },
   "outputs": [],
   "source": [
    "#token_dep_ + explain\n",
    "print([(token.text, token.dep_, spacy.explain(token.dep_)) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkjfHa-MVJ_6"
   },
   "source": [
    "spaCy add on:\n",
    "doc, any attributes, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raio9Hn0U2gW",
    "outputId": "d4631d8f-70c1-4243-f020-5dd693d4e685"
   },
   "outputs": [],
   "source": [
    "#any token_attributs; more: https://spacy.io/api/token#attributes\n",
    "print('token','-', 'lemma_', '-', 'lower_''-', 'pos_','-', 'tag_-', 'dep_','-', 'sentiment','-','is_alpha','-', 'is_stop')\n",
    "print('------------------------------------------------------------------------')   \n",
    "for i, token in zip(range (10), doc):\n",
    "    print(i, token.text,'-', token.lemma_, '-', token.lower_,'-', token.pos_,'-', token.tag_,'-',\n",
    "          token.dep_,'-', token.sentiment,'-', token.is_alpha,'-', token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yl9O9XACUuv7",
    "outputId": "82a8107a-9c27-4964-a190-b77c4e7aef36"
   },
   "outputs": [],
   "source": [
    "#Base Noun Phrases - flat phrases whose head is based on a noun\n",
    "for chunk in doc[0:10].noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhLbBQO-UnwG",
    "outputId": "f6ebba69-92ea-43d8-cf0a-8e7bf9bbf66c"
   },
   "outputs": [],
   "source": [
    "# For the first 100 words, print out the text, tag, and explanation of the tag.\n",
    "\n",
    "# If you want to format things better, try using f strings! For example, look at the following:\n",
    "# for number in range(0, 100):\n",
    "#   print(f\"Original Number: {number}, {' ':{10}} Original Number times 2: {number * 10}\") \n",
    "\n",
    "i = 0\n",
    "for i, token in zip(range(99),doc):\n",
    "    print(f\"{token}\\t{' ':{7}}\\t{token.tag_}\\t\\t{' ':{8}}{spacy.explain(token.tag_)}\")\n",
    "\n",
    "# {' ':{10} or \\t ---> it won't be very nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BiK6Tce_Uka_",
    "outputId": "2f110fa5-8b21-4df1-c210-c0b6408bffff"
   },
   "outputs": [],
   "source": [
    "#Count the frequencies of a given attribute with ORTH\n",
    "print(doc.count_by(ORTH))\n",
    "print(len(doc.count_by(ORTH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNnPJNT4UhjC",
    "outputId": "e9a00bd4-703b-4dd1-d97f-7c814669313c"
   },
   "outputs": [],
   "source": [
    "#other way\n",
    "\n",
    "dict_tags={}\n",
    "for token in doc:\n",
    "  #print(token,token.tag_)\n",
    "  try:\n",
    "    dict_tags[token.tag_].append(token.text)\n",
    "  except:\n",
    "    dict_tags[token.tag_]=[token.text]\n",
    "\n",
    "print(dict_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wY4TdQCUZk8",
    "outputId": "6703f899-3087-43f3-b80d-a442901ab8eb"
   },
   "outputs": [],
   "source": [
    "tag_labels = []\n",
    "for key in doc:\n",
    "    while token.head != token:\n",
    "        tag_labels.append(token.tag_)\n",
    "        token = token.head\n",
    "print(tag_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7Hc0dI7UW2N",
    "outputId": "415d1c78-b915-4bda-b3b1-b490e03fe8f7"
   },
   "outputs": [],
   "source": [
    "tags_count=[]\n",
    "for value in dict_tags.values():\n",
    "    tags_count.append(len(value))\n",
    "print(tags_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "8DRc3ZUaUQ_Q",
    "outputId": "f085d651-69c6-414b-c530-e88be4ba7a63"
   },
   "outputs": [],
   "source": [
    "# Visualize the POS tags of the first sentence in the original text file with displacy.\n",
    "\n",
    "#sentence\n",
    "sents = list(doc.sents)\n",
    "print(len(sents))\n",
    "sents[0]\n",
    "print(sents[0])\n",
    "\n",
    "displacy.render(sents[0],style=\"dep\" ,jupyter=True, options = {'distance' : 100})\n",
    "plt.savefig(path_plot + 'POS_tag.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "jEeJ7YOMDFLj",
    "outputId": "1306f4f0-6c3c-4f35-bb8b-9ed19c2fe97d"
   },
   "outputs": [],
   "source": [
    "# Visualize the Key_words and your classification of the first sentence with displacy.\n",
    "displacy.render(sents[0],style=\"ent\" ,jupyter=True)\n",
    "plt.savefig(path_plot + diagram + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqcwMrENC03w",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "copy from:\n",
    "https://shirishkadam.com/2017/07/03/nlp-question-classification-using-support-vector-machines-spacyscikit-learnpandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi9rWQ4F-J_P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_question (question, qclass, en_nlp):\n",
    "    en_doc = nlp (u, ' ' + question)\n",
    "    sent_list = list (en_doc.sents) \n",
    "    sent = sent_list [0] \n",
    "    wh_bi_gram = [] \n",
    "    root_token = \"\" \n",
    "    wh_pos = \"\" \n",
    "    wh_nbor_pos = \"\" \n",
    "    wh_word = \"\" \n",
    "    for sent in token:\n",
    "        if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP $\" or token.tag_ == \"WRB\":\n",
    "            wh_pos = token .tag_ \n",
    "            wh_word = token.text \n",
    "            wh_bi_gram.append (token.text) \n",
    "            wh_bi_gram.append (str (en_doc [token.i + 1])) \n",
    "            wh_nbor_pos = en_doc [token.i + 1].tag_ \n",
    "        if token.dep_ == \"ROOT\":\n",
    "            root_token = token.tag_ \n",
    "\n",
    "    write_each_record_to_csv (wh_pos, wh_word, wh_bi_gram, wh_nbor_pos, root_token)\n",
    "\n",
    "process_question (df_train['question'][50])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " eda_1_part of speech tags.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf_test38",
   "language": "python",
   "name": "tf_test38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
